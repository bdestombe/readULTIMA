# -*- coding: utf-8 -*-
"""
Created on Fri Apr 08 16:34:12 2016

Edited on my birthday 15 aug 2016 to support new version

@author: Bas des Tombe, bdestombe@gmail.com

To process files generated by the ULTIMA, by Silixa. New version!

Reads all the xml files from the specified folder. The data is merged into a
single netCDF file. That data usually contains:
LAF: length along fiber
ST: Stokes intensity
AST: anti-Stokes intensity
REV-ST: reverse Stokes intensity
REV-AST: reverse anti-Stokes intensity
TMP: By the device calibrated temperature

The meta-data from the first (alphabetically chosen) xml
file is added to the netCDF file, containing information on the method of
calibration some discretizations. Good practice to keep it all together

This should be considered step 1 of data processsing. This netCDF-file can be
used to create an actual dataset, including coordinates complying with
netCDF conventions CF. cfconventions.org/cf-conventions/cf-conventions.html

Vertical temperature profiles can use 'timeSeriesProfile', chp 5.2 of
cfconventions. Having a single spatial coordinate and a height above surface
attribute, one file per section.

Next steps:
- data was recorded for the entire cable. split this netCDF file into sections
    needed
- Add meta data according to CF conventions

"""
import pandas as pd
import numpy as np
import xmltodict
import os
import glob
from collections import OrderedDict as od
from xml.etree import ElementTree
import xarray as xr  # conda install xarray




# In[]: Define functions
def read_meta(filename, sep):
    def metakey(meta, dic_to_parse, prefix, sep):
        for key in dic_to_parse:
            if prefix == "":
                prefix_parse = key.replace('@', '')
            else:
                prefix_parse = sep.join([prefix, key.replace('@', '')])

            if prefix_parse == sep.join(('logData', 'data')):  # u'logData:data':
                continue

            if hasattr(dic_to_parse[key], 'keys'):
                meta.update(metakey(meta, dic_to_parse[key], prefix_parse, sep))

            elif isinstance(dic_to_parse[key], list):
                for ival, val in enumerate(dic_to_parse[key]):
                    num_key = prefix_parse + '_' + str(ival)
                    meta.update(metakey(meta, val, num_key, sep))
            else:

                meta[prefix_parse] = dic_to_parse[key]

        return meta

    with open(filename) as fh:
        doc_ = xmltodict.parse(fh.read())

    if u'wellLogs' in doc_.keys():
        doc = doc_[u'wellLogs'][u'wellLog']
    else:
        doc = doc_[u'logs'][u'log']

    return metakey(od(), doc, '', sep)


def find_dimensions(filename):
    # Returns nitem and nx
    with open(filename) as fh:
        doc = xmltodict.parse(fh.read())[u'logs'][u'log']

    return (len(doc[u'logCurveInfo']), len(doc[u'logData']['data']))


def grab_data(filelist, sep):
    # Obtain meta data from the first file
    meta = read_meta(filelist[0], sep)

    nitem, nx = find_dimensions(filelist[0])
    ntime = len(filelist)

    # Lookup recorded item names
    ddict = []
    for iitem in xrange(nitem):
        key = u'logCurveInfo_' + str(iitem) + sep + 'mnemonic'
        ddict.append((str(meta[key]), np.float32))

    # Allocate data and time arrays
    ddtype = np.dtype(ddict)
    array = np.recarray((nx, ntime), dtype=ddtype)
    timearr = np.zeros((ntime), dtype=[('minDateTimeIndex', 'S24'),
                                       ('maxDateTimeIndex', 'S24'),
                                       ('filename_tstamp', 'S17')])

    # print summary
    print '%s files were found, each representing a single timestep' %ntime
    print '%s recorded vars were found: ' %nitem + ', '.join(ddtype.names)
    print 'Recorded at %s points along the cable' %nx


    # grab data from all *.xml files in filelist
    for it, file_ in enumerate(filelist):
        print 'processing file ' + str(it + 1) + ' out of ' + str(ntime)
        with open(file_, 'r') as fh:
            tree = ElementTree.parse(fh)

            for ix, node in enumerate(tree.iter('{http://www.witsml.org/schemas/1series}data')):
                array[ix,it] = np.array(node.text.split(','), dtype = np.float32)

            timearr[it]['minDateTimeIndex'] = tree._root[0][3].text
            timearr[it]['minDateTimeIndex'] = tree._root[0][4].text

            file_name = os.path.split(file_)[1]
            timearr[it]['filename_tstamp'] = file_name[10:-4]  # hack

    return (array, timearr, meta)


# In[]: User input
# In: Folder name containing xml files
path = r"C:\Users\Bas\Downloads\ultimatest2"

# Out: Location of the netCDF file
path_nc = r"C:\Users\Bas\Downloads\ultima_test"
fn_nc = 'DTS2.nc'

# Timezone info
timezone_oryx = 'Europe/Amsterdam'
timezone_netCDF = 'UTC'  # Leave this at UTC, according to CF conventions

# In[]: Read all measurement XMLs
filelist = glob.glob(os.path.join(path, '*.xml'))
sep = ':'  # common practice in netCDF world

array, timearr, meta = grab_data(filelist, sep)

# In[]:
# Use xarray to export to netCDF.
index_time = pd.to_datetime(timearr['filename_tstamp'],
                            format='%Y%m%d%H%M%S%f').tz_localize(
                            tz=timezone_oryx).tz_convert(timezone_netCDF)
index_x = array.LAF[:, 0]  # assuming length along fiber doesnt change over t

dataset_dict = {}
for name in array.dtype.names:
    dataset_dict[name] = (['x', 'time'], array[name])

coords = {'x': index_x,
          'time': index_time.astype('datetime64[ns]')  # in UTC
          }

ds = xr.Dataset(dataset_dict, coords=coords, attrs=meta)

ds.to_netcdf(os.path.join(path_nc, fn_nc), format='NETCDF3_CLASSIC')

# Read the same netCDF file
#ds_disk = xr.open_dataset(os.path.join(path_nc, fn_nc))

# In[]
# plot
#import matplotlib.pyplot as plt
#plt.imshow(ds['TMP']._variable._data.T[1500:], vmin=3, vmax=20)